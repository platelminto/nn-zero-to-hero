{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['emma',\n 'olivia',\n 'ava',\n 'isabella',\n 'sophia',\n 'charlotte',\n 'mia',\n 'amelia',\n 'harper',\n 'evelyn']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "words = open('../data/names.txt', 'r').read().splitlines()\n",
    "words[:10]\n",
    "\n",
    "# words = [\"giorgio\", \"gina\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('z', 'w'),\n ('z', 'x'),\n ('z', 'y'),\n ('z', 'z'),\n ('.', 'a'),\n ('.', 'b'),\n ('.', 'c'),\n ('.', 'd'),\n ('.', 'e'),\n ('.', 'f'),\n ('.', 'g'),\n ('.', 'h'),\n ('.', 'i'),\n ('.', 'j'),\n ('.', 'k'),\n ('.', 'l'),\n ('.', 'm'),\n ('.', 'n'),\n ('.', 'o'),\n ('.', 'p'),\n ('.', 'q'),\n ('.', 'r'),\n ('.', 's'),\n ('.', 't'),\n ('.', 'u'),\n ('.', 'v'),\n ('.', 'w'),\n ('.', 'x'),\n ('.', 'y'),\n ('.', 'z')]"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = list(itertools.product(chars, repeat=2))\n",
    "inputs += [('.', c) for c in chars]\n",
    "inputs[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "702"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {s:i for i, s in enumerate(inputs)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "stoo = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoo['.'] = 0\n",
    "otos = {i:s for s, i in stoo.items()}\n",
    "n_tokens = len(stoi)\n",
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.ones((n_tokens, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs)-2):\n",
    "        # print(tuple(chs[i:i+2]), chs[i+2])\n",
    "        ixi = stoi[tuple(chs[i:i+2])]\n",
    "        ixo = stoo[chs[i+2]]\n",
    "        N[ixi, ixo] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float()\n",
    "P /= P.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.3341, 0.0215, 0.0047, 0.0028, 0.0075, 0.0033, 0.0014, 0.0005, 0.2385,\n        0.0163, 0.0005, 0.0033, 0.0229, 0.0187, 0.2235, 0.0005, 0.0009, 0.0028,\n        0.0359, 0.0322, 0.0103, 0.0005, 0.0028, 0.0009, 0.0014, 0.0028, 0.0098])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[676]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ala.\n",
      "ka.\n",
      "makacarielylan.\n",
      "samaroswren.\n",
      "auoira.\n",
      "bayse.\n",
      "elyn.\n",
      "dijalee.\n",
      "belain.\n",
      "arinicto.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 14)\n",
    "for i in range(10):\n",
    "    ix = stoi[('.', 'a')] + torch.multinomial(\n",
    "        N[stoi[('.', 'a')]:stoi[('.', 'z')]+1].float().sum(1),\n",
    "        num_samples=1,\n",
    "        replacement=True,\n",
    "        generator=g\n",
    "    ).item()\n",
    "    out = [itos[ix][1]]\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ixo = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(otos[ixo])\n",
    "        # print(itos[ix], ix)\n",
    "        # print(otos[ixo], ixo)\n",
    "        if otos[ixo] == '.':\n",
    "            break\n",
    "        ix = stoi[itos[ix][1], otos[ixo]]\n",
    "\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL: to maximise the likelihood of the data w.r.t. the model parameters\n",
    "# (statistical modelling). what mean this ? we can't realistically get 100%\n",
    "# probability. let's say:\n",
    "# our training set is ['giorgio', 'gina']:\n",
    "# there is no way to have 100% probability for what follows 'i',\n",
    "# since it could either be bigram 'io' or 'in'. But we want to maximise the\n",
    "# chance of them showing up, and correctly: as close to 1/3 for 'in' and 2/3\n",
    "# for 'io' as possible. If we make 'io' much higher, the likelihood of the\n",
    "# rest of the data ('g<in>a') showing up becomes much lower, and so the\n",
    "# overall quality of the model is lower. MAXIMISE THE LIKELIHOOD OF THE\n",
    "# DATA WITH RESPECT TO THE MODEL PARAMETERS. CAN'T HAVE ALL 1s AS\n",
    "# PROBABILITIES SINCE THEY WILL CLASH - JUST LOOKING FOR WHAT'S BEST. WITH\n",
    "# BIGRAMS DATA, THE CALCULATED LOSS W.R.T TRAINING DATA IS AS GOOD AS IT'S\n",
    "# GOING TO GET.\n",
    "# so we want to maximise the product of all probabilities.\n",
    "# i.e. maximise the log likelihood (since probs are 0-1, the logs are gonna\n",
    "# be negative, and we want to get close to 0)\n",
    "# i.e. minimise negative log likelihood (positive number == loss, want it to\n",
    "# be as low as possible)\n",
    "# i.e. minimise average negative log likelihood.\n",
    "\n",
    "# The model is good when the probabilities for each bigram are high. Why?\n",
    "# See above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-410414.9688)\n",
      "nll=tensor(410414.9688)\n",
      "2.092747449874878\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs)-2):\n",
    "        ix1, ix2 = stoi[tuple(chs[i:i+2])], stoo[chs[i+2]]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.functional import cross_entropy, nll_loss, log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# create the training set of all the trigrams\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs)-2):\n",
    "        ix1 = stoi[tuple(chs[i:i+2])]\n",
    "        ix2 = stoo[chs[i+2]]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples:', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 100\n",
    "R = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, weights):\n",
    "    # xenc = F.one_hot(inputs, num_classes=n_tokens).float() # input to the NN: one-hot enc\n",
    "    # logits = xenc @ weights # predict log-counts\n",
    "    logits = weights[inputs]\n",
    "    counts = logits.exp() # convert to counts (analogous to N above)\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next char\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_nll(probs, n_inputs, labels):\n",
    "    return -probs[torch.arange(n_inputs), labels].log().mean() + R*(W**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ixs, dev_ixs, test_ixs = random_split(range(num), [0.8, 0.1, 0.1])\n",
    "train_xs, train_ys = xs[train_ixs], ys[train_ixs]\n",
    "dev_xs, dev_ys = xs[dev_ixs], ys[dev_ixs]\n",
    "test_xs, test_ys = xs[test_ixs], ys[test_ixs]\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((n_tokens, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([702, 27])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1829\n",
      "2.1825\n",
      "2.1820\n",
      "2.1815\n",
      "2.1811\n",
      "2.1806\n",
      "2.1802\n",
      "2.1797\n",
      "2.1793\n",
      "2.1789\n",
      "2.1784\n",
      "2.1780\n",
      "2.1776\n",
      "2.1771\n",
      "2.1767\n",
      "2.1763\n",
      "2.1759\n",
      "2.1755\n",
      "2.1751\n",
      "2.1747\n",
      "2.1743\n",
      "2.1739\n",
      "2.1735\n",
      "2.1731\n",
      "2.1727\n",
      "2.1723\n",
      "2.1720\n",
      "2.1716\n",
      "2.1712\n",
      "2.1708\n",
      "2.1705\n",
      "2.1701\n",
      "2.1697\n",
      "2.1694\n",
      "2.1690\n",
      "2.1687\n",
      "2.1683\n",
      "2.1680\n",
      "2.1676\n",
      "2.1673\n",
      "2.1669\n",
      "2.1666\n",
      "2.1663\n",
      "2.1659\n",
      "2.1656\n",
      "2.1653\n",
      "2.1650\n",
      "2.1646\n",
      "2.1643\n",
      "2.1640\n",
      "2.1637\n",
      "2.1634\n",
      "2.1631\n",
      "2.1627\n",
      "2.1624\n",
      "2.1621\n",
      "2.1618\n",
      "2.1615\n",
      "2.1612\n",
      "2.1609\n",
      "2.1606\n",
      "2.1604\n",
      "2.1601\n",
      "2.1598\n",
      "2.1595\n",
      "2.1592\n",
      "2.1589\n",
      "2.1586\n",
      "2.1584\n",
      "2.1581\n",
      "2.1578\n",
      "2.1575\n",
      "2.1573\n",
      "2.1570\n",
      "2.1567\n",
      "2.1565\n",
      "2.1562\n",
      "2.1560\n",
      "2.1557\n",
      "2.1554\n",
      "2.1552\n",
      "2.1549\n",
      "2.1547\n",
      "2.1544\n",
      "2.1542\n",
      "2.1539\n",
      "2.1537\n",
      "2.1534\n",
      "2.1532\n",
      "2.1529\n",
      "2.1527\n",
      "2.1525\n",
      "2.1522\n",
      "2.1520\n",
      "2.1518\n",
      "2.1515\n",
      "2.1513\n",
      "2.1511\n",
      "2.1508\n",
      "2.1506\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "    probs = forward_pass(train_xs, W)\n",
    "\n",
    "    # print(f'{W.shape=}')\n",
    "    # print(f'{xenc.shape=}')\n",
    "    # print(f'{probs.shape=}')\n",
    "    # print(f'{ys.shape=}')\n",
    "    loss = mean_nll(probs, len(train_xs), train_ys)\n",
    "    # second arg normalises to bring Ws to 0. equivalent to (N+1) from above.\n",
    "    # unsure why we do this here though.\n",
    "    print(f'{loss:=.4f}')\n",
    "\n",
    "    # second-last 2 lines are softmax = go from logits (some positive & negative\n",
    "    # numbers) to a probability distribution.\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set gradient to 0\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -L * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_loss.item()=2.1896\n",
      "test_loss.item()=2.1832\n"
     ]
    }
   ],
   "source": [
    "dev_loss = mean_nll(forward_pass(dev_xs, W), len(dev_xs), dev_ys)\n",
    "print(f'{dev_loss.item()=:.4f}')\n",
    "test_loss = mean_nll(forward_pass(test_xs, W), len(test_xs), test_ys)\n",
    "print(f'{test_loss.item()=:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax & Sigmoid are output layers. Softmax returns a probability distribution - increasing the\n",
    "# likelihood of one class reduces the likelihood of others (= multiclass classification. Only 1 class).\n",
    "# Sigmoid gives 0-1 per each class. They could sum to greater than 1. (= multilabel classification.\n",
    "# multiple classes could be valid).\n",
    "\n",
    "# You can mix and match output layers with cost functions.\n",
    "\n",
    "# Cross-entropy and NLL (negative log-likelihood, similar to maximum likelihood estimation) are cost\n",
    "# functions. In multiclass classification, they are equivalent, because cross-entropy over a softmax\n",
    "# output layer (i.e. a probability distribution over all examples) is the same as NLL. In multi-label\n",
    "# classification, you can't have a prob. distribution over all examples (like the NLL equation), so\n",
    "# there is no equivalence. You can do binary cross-entropy here, interpreting each output neuron as\n",
    "# its own probability p (YES) and 1-p (NO), then sum over all these probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy and NLL both reward network for high probabilities in correct output classes,\n",
    "# but Cross-entropy also penalises network for high probabilities in the wrong classes. NLL alone\n",
    "# doesn't do this, but we achieve something similar using the regularisation factor from above\n",
    "# (which pushes all weights to 0). I think this only happens in binary cross-entropy. See above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN approach is converging on the final loss of the table-probability approach\n",
    "# because the table approach is as good as we can get it. Essentially, it is\n",
    "# perfect. The problem arises when we expand from bigrams. Tri-grams, 4-grams,\n",
    "# the tables for these become unwieldy (4-gram: need all 3-char combination\n",
    "# inputs leading to an output). It is un-scalable.\n",
    "# NNs (and the gradient descent approach) are scalable - we just change the\n",
    "# way to get the logits (the forward pass), and everything else stays the same.\n",
    "# Stick a softmax to the end of it, gradient descent, adjust weights. Easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOW OK\n",
    "# The W we end up with is the table N we calculated by counting from above!!\n",
    "# The comparison: Matrix multiplying a one-hot encoded vector by a matrix\n",
    "# is equivalent to 'selecting' the row represented by that vector (if we\n",
    "# had 5 as the one-hot, we select row 5 of W). Which is what we do with N:\n",
    "# based on the first character, pick that row in the table, and you have a row\n",
    "# of counts for each character that could follow it.\n",
    "# Damnnnnnnnnn.\n",
    "# Well, W is the log-counts. So W.exp() kinda equals N. Difference is, we\n",
    "# counted everything to get to N, while we started with random values and\n",
    "# grad-descented to what W is to get the right logcounts/counts/probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
