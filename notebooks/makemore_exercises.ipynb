{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['emma',\n 'olivia',\n 'ava',\n 'isabella',\n 'sophia',\n 'charlotte',\n 'mia',\n 'amelia',\n 'harper',\n 'evelyn']"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "words = open('../data/names.txt', 'r').read().splitlines()\n",
    "words[:10]\n",
    "\n",
    "# words = [\"giorgio\", \"gina\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "outputs": [
    {
     "data": {
      "text/plain": "[('a', 'a'),\n ('a', 'b'),\n ('a', 'c'),\n ('a', 'd'),\n ('a', 'e'),\n ('a', 'f'),\n ('a', 'g'),\n ('a', 'h'),\n ('a', 'i'),\n ('a', 'j')]"
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = list(itertools.product(chars, repeat=2))\n",
    "inputs += [('.', c) for c in chars]\n",
    "inputs[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [
    {
     "data": {
      "text/plain": "702"
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {s:i for i, s in enumerate(inputs)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "stoo = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoo['.'] = 0\n",
    "otos = {i:s for s, i in stoo.items()}\n",
    "n_tokens = len(stoi)\n",
    "n_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [],
   "source": [
    "N = torch.zeros((n_tokens, 27), dtype=torch.int32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs)-2):\n",
    "        ixi = stoi[tuple(chs[i:i+2])]\n",
    "        ixo = stoo[chs[i+2]]\n",
    "        N[ixi, ixo] += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [],
   "source": [
    "P = N.float()\n",
    "P /= P.sum(1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0000, 0.0469, 0.0431, 0.0070, 0.0830, 0.0125, 0.0048, 0.0039, 0.0206,\n        0.0349, 0.0061, 0.0170, 0.1433, 0.0871, 0.1413, 0.0023, 0.0039, 0.0020,\n        0.1093, 0.0440, 0.0163, 0.0345, 0.0551, 0.0014, 0.0061, 0.0392, 0.0345])"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[676]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alashdiyassann.\n",
      "annleah.\n",
      "agday.\n",
      "asiabduliah.\n",
      "ah.\n"
     ]
    }
   ],
   "source": [
    "# g = torch.Generator().manual_seed(2147483647)\n",
    "start_letter = 'a'\n",
    "for i in range(5):\n",
    "    out = [start_letter]\n",
    "    ix = stoi[('.', start_letter)]\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ixo = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "        out.append(otos[ixo])\n",
    "        # print(itos[ix], ix)\n",
    "        # print(otos[ixo], ixo)\n",
    "        if otos[ixo] == '.':\n",
    "            break\n",
    "        ix = stoi[itos[ix][1], otos[ixo]]\n",
    "\n",
    "    print(''.join(out))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# GOAL: to maximise the likelihood of the data w.r.t. the model parameters\n",
    "# (statistical modelling). what mean this ? we can't realistically get 100%\n",
    "# probability. let's say:\n",
    "# our training set is ['giorgio', 'gina']:\n",
    "# there is no way to have 100% probability for what follows 'i',\n",
    "# since it could either be bigram 'io' or 'in'. But we want to maximise the\n",
    "# chance of them showing up, and correctly: as close to 1/3 for 'in' and 2/3\n",
    "# for 'io' as possible. If we make 'io' much higher, the likelihood of the\n",
    "# rest of the data ('g<in>a') showing up becomes much lower, and so the\n",
    "# overall quality of the model is lower. MAXIMISE THE LIKELIHOOD OF THE\n",
    "# DATA WITH RESPECT TO THE MODEL PARAMETERS. CAN'T HAVE ALL 1s AS\n",
    "# PROBABILITIES SINCE THEY WILL CLASH - JUST LOOKING FOR WHAT'S BEST. WITH\n",
    "# BIGRAMS DATA, THE CALCULATED LOSS W.R.T TRAINING DATA IS AS GOOD AS IT'S\n",
    "# GOING TO GET.\n",
    "# so we want to maximise the product of all probabilities.\n",
    "# i.e. maximise the log likelihood (since probs are 0-1, the logs are gonna\n",
    "# be negative, and we want to get close to 0)\n",
    "# i.e. minimise negative log likelihood (positive number == loss, want it to\n",
    "# be as low as possible)\n",
    "# i.e. minimise average negative log likelihood.\n",
    "\n",
    "# The model is good when the probabilities for each bigram are high. Why?\n",
    "# See above.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-303102.5938)\n",
      "nll=tensor(303102.5938)\n",
      "2.295414447784424\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for i in range(len(w)-2):\n",
    "        ix1, ix2 = stoi[tuple(w[i:i+2])], stoo[w[i+2]]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.functional import cross_entropy, nll_loss, log_softmax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 196113\n"
     ]
    }
   ],
   "source": [
    "# create the training set of all the trigrams\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs)-2):\n",
    "        ix1 = stoi[''.join(chs[i:i+2])]\n",
    "        ix2 = stoo[chs[i+2]]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples:', num)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "L = 100\n",
    "R = 0.01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [],
   "source": [
    "def forward_pass(inputs, weights):\n",
    "    # xenc = F.one_hot(inputs, num_classes=n_tokens).float() # input to the NN: one-hot enc\n",
    "    # logits = xenc @ weights # predict log-counts\n",
    "    logits = weights[inputs]\n",
    "    counts = logits.exp() # convert to counts (analogous to N above)\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next char\n",
    "\n",
    "    return probs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "def mean_nll(probs, n_inputs, labels):\n",
    "    return -probs[torch.arange(n_inputs), labels].log().mean() # + R*(W**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [],
   "source": [
    "train_ixs, dev_ixs, test_ixs = random_split(range(num), [0.8, 0.1, 0.1])\n",
    "train_xs, train_ys = xs[train_ixs], ys[train_ixs]\n",
    "dev_xs, dev_ys = xs[dev_ixs], ys[dev_ixs]\n",
    "test_xs, test_ys = xs[test_ixs], ys[test_ixs]\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((n_tokens, 27), generator=g, requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0842\n",
      "2.0842\n",
      "2.0842\n",
      "2.0842\n",
      "2.0842\n",
      "2.0842\n",
      "2.0842\n",
      "2.0841\n",
      "2.0841\n",
      "2.0841\n",
      "2.0841\n",
      "2.0841\n",
      "2.0841\n",
      "2.0841\n",
      "2.0840\n",
      "2.0840\n",
      "2.0840\n",
      "2.0840\n",
      "2.0840\n",
      "2.0840\n",
      "2.0840\n",
      "2.0839\n",
      "2.0839\n",
      "2.0839\n",
      "2.0839\n",
      "2.0839\n",
      "2.0839\n",
      "2.0839\n",
      "2.0838\n",
      "2.0838\n",
      "2.0838\n",
      "2.0838\n",
      "2.0838\n",
      "2.0838\n",
      "2.0838\n",
      "2.0837\n",
      "2.0837\n",
      "2.0837\n",
      "2.0837\n",
      "2.0837\n",
      "2.0837\n",
      "2.0837\n",
      "2.0836\n",
      "2.0836\n",
      "2.0836\n",
      "2.0836\n",
      "2.0836\n",
      "2.0836\n",
      "2.0836\n",
      "2.0835\n",
      "2.0835\n",
      "2.0835\n",
      "2.0835\n",
      "2.0835\n",
      "2.0835\n",
      "2.0835\n",
      "2.0834\n",
      "2.0834\n",
      "2.0834\n",
      "2.0834\n",
      "2.0834\n",
      "2.0834\n",
      "2.0834\n",
      "2.0834\n",
      "2.0833\n",
      "2.0833\n",
      "2.0833\n",
      "2.0833\n",
      "2.0833\n",
      "2.0833\n",
      "2.0833\n",
      "2.0832\n",
      "2.0832\n",
      "2.0832\n",
      "2.0832\n",
      "2.0832\n",
      "2.0832\n",
      "2.0832\n",
      "2.0831\n",
      "2.0831\n",
      "2.0831\n",
      "2.0831\n",
      "2.0831\n",
      "2.0831\n",
      "2.0831\n",
      "2.0831\n",
      "2.0830\n",
      "2.0830\n",
      "2.0830\n",
      "2.0830\n",
      "2.0830\n",
      "2.0830\n",
      "2.0830\n",
      "2.0829\n",
      "2.0829\n",
      "2.0829\n",
      "2.0829\n",
      "2.0829\n",
      "2.0829\n",
      "2.0829\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "    probs = forward_pass(xs, W)\n",
    "\n",
    "    # print(f'{W.shape=}')\n",
    "    # print(f'{xenc.shape=}')\n",
    "    # print(f'{probs.shape=}')\n",
    "    # print(f'{ys.shape=}')\n",
    "    loss = mean_nll(probs, len(xs), ys)\n",
    "    # second arg normalises to bring Ws to 0. equivalent to (N+1) from above.\n",
    "    # unsure why we do this here though.\n",
    "    print(f'{loss:=.4f}')\n",
    "\n",
    "    # second-last 2 lines are softmax = go from logits (some positive & negative\n",
    "    # numbers) to a probability distribution.\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set gradient to 0\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -L * W.grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_loss.item()=3.6502\n",
      "test_loss.item()=3.6595\n"
     ]
    }
   ],
   "source": [
    "dev_loss = mean_nll(forward_pass(dev_xs, W), len(dev_xs), dev_ys)\n",
    "print(f'{dev_loss.item()=:.4f}')\n",
    "test_loss = mean_nll(forward_pass(test_xs, W), len(test_xs), test_ys)\n",
    "print(f'{test_loss.item()=:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Softmax & Sigmoid are output layers. Softmax returns a probability distribution - increasing the\n",
    "# likelihood of one class reduces the likelihood of others (= multiclass classification. Only 1 class).\n",
    "# Sigmoid gives 0-1 per each class. They could sum to greater than 1. (= multilabel classification.\n",
    "# multiple classes could be valid).\n",
    "\n",
    "# You can mix and match output layers with cost functions.\n",
    "\n",
    "# Cross-entropy and NLL (negative log-likelihood, similar to maximum likelihood estimation) are cost\n",
    "# functions. In multiclass classification, they are equivalent, because cross-entropy over a softmax\n",
    "# output layer (i.e. a probability distribution over all examples) is the same as NLL. In multi-label\n",
    "# classification, you can't have a prob. distribution over all examples (like the NLL equation), so\n",
    "# there is no equivalence. You can do binary cross-entropy here, interpreting each output neuron as\n",
    "# its own probability p (YES) and 1-p (NO), then sum over all these probabilities."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cross-entropy and NLL both reward network for high probabilities in correct output classes,\n",
    "# but Cross-entropy also penalises network for high probabilities in the wrong classes. NLL alone\n",
    "# doesn't do this, but we achieve something similar using the regularisation factor from above\n",
    "# (which pushes all weights to 0)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# NN approach is converging on the final loss of the table-probability approach\n",
    "# because the table approach is as good as we can get it. Essentially, it is\n",
    "# perfect. The problem arises when we expand from bigrams. Tri-grams, 4-grams,\n",
    "# the tables for these become unwieldy (4-gram: need all 3-char combination\n",
    "# inputs leading to an output). It is un-scalable.\n",
    "# NNs (and the gradient descent approach) are scalable - we just change the\n",
    "# way to get the logits (the forward pass), and everything else stays the same.\n",
    "# Stick a softmax to the end of it, gradient descent, adjust weights. Easy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# WOW OK\n",
    "# The W we end up with is the table N we calculated by counting from above!!\n",
    "# The comparison: Matrix multiplying a one-hot encoded vector by a matrix\n",
    "# is equivalent to 'selecting' the row represented by that vector (if we\n",
    "# had 5 as the one-hot, we select row 5 of W). Which is what we do with N:\n",
    "# based on the first character, pick that row in the table, and you have a row\n",
    "# of counts for each character that could follow it.\n",
    "# Damnnnnnnnnn.\n",
    "# Well, W is the log-counts. So W.exp() kinda equals N. Difference is, we\n",
    "# counted everything to get to N, while we started with random values and\n",
    "# grad-descented to what W is to get the right logcounts/counts/probabilities."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
